{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Q_Learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8P_laMcSQNk"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "import time"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpG5Q7_XSdPw",
        "outputId": "f1f70500-b68f-40a5-d2c8-60d0db8642f0"
      },
      "source": [
        "# Initialize Q-value table randomly\n",
        "q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "print(q_table)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3xVez-WTeww"
      },
      "source": [
        "def q_learning(env, num_episodes, num_steps_per_episode, learning_rate, gamma, max_epsilon, min_epsilon, epsilon_decay_rate):\n",
        "    q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "    rewards_all = []\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "\n",
        "        reward_episode = 0.0\n",
        "        done = False\n",
        "        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-epsilon_decay_rate*episode)\n",
        "        for step in range(num_steps_per_episode):\n",
        "            exploration = random.uniform(0,1)\n",
        "            if exploration < epsilon:\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                action = np.argmax(q_table[state, :])\n",
        "\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            q_table[state, action] = q_table[state, action] * (1 - learning_rate) + learning_rate * (reward + gamma * np.max(q_table[next_state,:]))\n",
        "\n",
        "            reward_episode += reward\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "        rewards_all.append(reward_episode)\n",
        "    print(f'Episode {episode} finished')\n",
        "    return q_table, rewards_all"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sarsa"
      ],
      "metadata": {
        "id": "ko_lWfcJJU67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def choose_action(env,epsilon,state,q_table,):\n",
        "  exploration = random.uniform(0,1)\n",
        "  if exploration < epsilon:\n",
        "      action = env.action_space.sample()\n",
        "  else:\n",
        "      action = np.argmax(q_table[state, :])\n",
        "  return action"
      ],
      "metadata": {
        "id": "YiFurcLsFhPm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sarsa(env, num_episodes, num_steps_per_episode, learning_rate, gamma, max_epsilon, min_epsilon, epsilon_decay_rate):\n",
        "  q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "  rewards_all = []\n",
        "  for episode in range(num_episodes):\n",
        "      state_1 = env.reset()\n",
        "\n",
        "      reward_episode = 0.0\n",
        "      done = False\n",
        "      epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-epsilon_decay_rate*episode)\n",
        "      action_1 = choose_action(env,epsilon,state_1,q_table)\n",
        "      for step in range(num_steps_per_episode):\n",
        "        state_2, reward, done, info = env.step(action_1)\n",
        "        action_2 = choose_action(env,epsilon,state_2,q_table)\n",
        "\n",
        "        q_table[state_1, action_1] = q_table[state_1, action_1] + learning_rate * (reward + gamma * q_table[state_2, action_2] - q_table[state_1, action_1])\n",
        "\n",
        "        state_1 = state_2\n",
        "        action_1 = action_2\n",
        "\n",
        "        reward_episode += reward\n",
        "\n",
        "        if done:\n",
        "              break\n",
        "      rewards_all.append(reward_episode)\n",
        "  print(f'Episode {episode} finished')\n",
        "  return q_table, rewards_all\n"
      ],
      "metadata": {
        "id": "zyArxFHQEgAH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGopsD0IWpDO"
      },
      "source": [
        "def play(env, q_table, render=False):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    steps = 0\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = np.argmax(q_table[state, :])\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        total_reward += reward\n",
        "        steps += 1\n",
        "        if render:\n",
        "            env.render()\n",
        "            time.sleep(0.2)\n",
        "            if not done:\n",
        "                display.clear_output(wait=True)\n",
        "        state = next_state\n",
        "\n",
        "    return (total_reward, steps)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def play_multiple_times(env, q_table, max_episodes):\n",
        "    success = 0\n",
        "    list_of_steps = []\n",
        "    for i in range(max_episodes):\n",
        "        total_reward, steps = play(env, q_table)\n",
        "\n",
        "        if total_reward > 0:\n",
        "            success += 1\n",
        "            list_of_steps.append(steps)\n",
        "\n",
        "    print(f'Number of successes: {success}/{max_episodes}')\n",
        "    print(f'Average number of steps: {np.mean(list_of_steps)}')"
      ],
      "metadata": {
        "id": "2l8BKi9TSqRe"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Env FrozenLake-v0"
      ],
      "metadata": {
        "id": "blgm20WbKKIX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xifGZ8j-SWPT"
      },
      "source": [
        "env = gym.make('FrozenLake-v0')"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFsyfXH5Ssd6"
      },
      "source": [
        "# Hyperparameters\n",
        "gamma = 0.99\n",
        "learning_rate = 0.1\n",
        "max_epsilon = 1.0\n",
        "min_epsilon = 0.01\n",
        "epsilon_decay_rate = 0.005\n",
        "\n",
        "num_episodes = 20000\n",
        "num_steps_per_episode = 100"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q_table, rewards_all = q_learning(env, num_episodes, num_steps_per_episode, learning_rate, gamma, max_epsilon, min_epsilon, epsilon_decay_rate)\n",
        "play_multiple_times(env, q_table, 1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hs-EbCUUSvf2",
        "outputId": "c9a98ed4-4185-4ef5-f679-d39215561f0f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 19999 finished\n",
            "Number of successes: 713/1000\n",
            "Average number of steps: 37.842917251051894\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q_table_sar, rewards_all_sar = sarsa(env, num_episodes, num_steps_per_episode, learning_rate, gamma, max_epsilon, min_epsilon, epsilon_decay_rate)\n",
        "play_multiple_times(env, q_table_sar, 1000)"
      ],
      "metadata": {
        "id": "bm4CcsAzSx-f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f46fc867-5a51-43bd-966b-8b8466e52f55"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 19999 finished\n",
            "Number of successes: 758/1000\n",
            "Average number of steps: 38.66094986807388\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Env FrozenLake8x8-v0"
      ],
      "metadata": {
        "id": "dMGS6uD7KPvX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('FrozenLake8x8-v0')"
      ],
      "metadata": {
        "id": "H4ZpVeNYKj_D"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "gamma = 0.99\n",
        "learning_rate = 0.1\n",
        "max_epsilon = 1.0\n",
        "min_epsilon = 0.01\n",
        "epsilon_decay_rate = 0.00005\n",
        "\n",
        "num_episodes = 20000\n",
        "num_steps_per_episode = 400"
      ],
      "metadata": {
        "id": "0ugaESghtz1Y"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#q_learning\n",
        "q_table, rewards_all = q_learning(env, num_episodes, num_steps_per_episode, learning_rate, gamma, max_epsilon, min_epsilon, epsilon_decay_rate)\n",
        "play_multiple_times(env, q_table, 1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFBltF46KOX_",
        "outputId": "df3e2e78-4742-4d98-a569-f764a00cf258"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 19999 finished\n",
            "Number of successes: 857/1000\n",
            "Average number of steps: 97.34305717619603\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Sarsa\n",
        "q_table_sar, rewards_all_sar = sarsa(env, num_episodes, num_steps_per_episode, learning_rate, gamma, max_epsilon, min_epsilon, epsilon_decay_rate)\n",
        "play_multiple_times(env, q_table_sar, 1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWGQIPuEKUtK",
        "outputId": "2b5bfe54-0b29-4b0f-8b59-b7d374a09838"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 19999 finished\n",
            "Number of successes: 772/1000\n",
            "Average number of steps: 80.48056994818653\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Env Taxi-v3"
      ],
      "metadata": {
        "id": "Mb0-G-SnKVDO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('Taxi-v3')"
      ],
      "metadata": {
        "id": "zC8xKKlZLTkh"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "gamma = 0.99\n",
        "learning_rate = 0.1\n",
        "max_epsilon = 1.0\n",
        "min_epsilon = 0.01\n",
        "epsilon_decay_rate = 0.005\n",
        "\n",
        "num_episodes = 20000\n",
        "num_steps_per_episode = 100"
      ],
      "metadata": {
        "id": "o35-kX-4zQsB"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#q_learning\n",
        "q_table, rewards_all = q_learning(env, num_episodes, num_steps_per_episode, learning_rate, gamma, max_epsilon, min_epsilon, epsilon_decay_rate)\n",
        "play_multiple_times(env, q_table, 1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEWpUTrJLR_R",
        "outputId": "cd3ec49d-234c-42b6-e56c-fb99b9af4cef"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 19999 finished\n",
            "Number of successes: 1000/1000\n",
            "Average number of steps: 13.136\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Sarsa\n",
        "q_table_sar, rewards_all_sar = sarsa(env, num_episodes, num_steps_per_episode, learning_rate, gamma, max_epsilon, min_epsilon, epsilon_decay_rate)\n",
        "play_multiple_times(env, q_table_sar, 1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQIjHlSDLRBd",
        "outputId": "7d796208-fdda-4934-8088-041153b4ad02"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 19999 finished\n",
            "Number of successes: 1000/1000\n",
            "Average number of steps: 13.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nhận xét\n",
        "Q_learning và SarSa hoạt động ổn trên môi trường FrozenLake-v0 với số lượt thắng khá cao và SarSa có vẻ hoạt động tốt hơn Q_learning trên môi trường này với số lượt thắng lớn hơn với trung bình các bước đi tuy lớn hơn Q_learning nhưng không đáng kể chỉ xấp xỉ 1 bước đi. Nhưng đến với môi trường FrozenLake8x8-v0 có thể thấy môi trường này lớn gấp 4 lần môi trường FrozenLake-v0 và cả 2 thuật toán đều sẽ không hoạt động tốt với không gian trạng thái quá lớn, chính vì thế phải điểu chỉnh hyperparameters phù hợp (num_steps mỗi episode cao hơn, epsilon_decay_rate thấp). Ở môi trường này thì có vẻ Q_learning hoạt động tốt hơn với số lần chiến thắng nhiều hơn SarSa nhưng số bước đi trung bình để đạt chiến thắng khá cao. Đến với môi trường cuối cung Taxi-v3 thì cả 2 thuật toán đều hoạt động rất tốt với số lượt thắng tuyết đối và số bước đi trung bình của cả 2 thuật toán xấp xỉ nhau. "
      ],
      "metadata": {
        "id": "dGH0Iac3ybX2"
      }
    }
  ]
}